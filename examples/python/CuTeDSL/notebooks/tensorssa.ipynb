{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 17:01:49,625 - default - DEBUG - [setup_log] - /home/jeromeku/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/utils/logger.py:76 Logging setup!\n",
      "2025-05-14 17:01:49,627 - default - INFO - [<module>] - /home/jeromeku/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/utils/logger.py:90 Set up logger at /home/jeromeku/cutlass/cute_dsl_logs/20250514:17:01.log\n"
     ]
    }
   ],
   "source": [
    "import cutlass\n",
    "import cutlass.cute as cute\n",
    "from cutlass.cute.runtime import from_dlpack\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the TensorSSA in CuTe DSL\n",
    "\n",
    "This tutorial introduces what is the `TensorSSA` and why we need it. We also give some examples to show how to use `TensorSSA`.\n",
    "\n",
    "## What is TensorSSA\n",
    "\n",
    "`TensorSSA` is a Python class that represents a tensor value in Static Single Assignment (SSA) form within the CuTe DSL. You can think of it as a tensor residing in a (simulated) register.\n",
    "\n",
    "## Why TensorSSA\n",
    "\n",
    "`TensorSSA` encapsulates the underlying MLIR tensor value into an object that's easier to manipulate in Python. By overloading numerous Python operators (like `+`, `-`, `*`, `/`, `[]`, etc.), it allows users to express tensor computations (primarily element-wise operations and reductions) in a more Pythonic way. These element-wise operations are then translated into optimized vectorization instructions.\n",
    "\n",
    "It's part of the CuTe DSL, serving as a bridge between the user-described computational logic and the lower-level MLIR IR, particularly for representing and manipulating register-level data.\n",
    "\n",
    "## When to use TensorSSA\n",
    "\n",
    "`TensorSSA` is primarily used in the following scenarios:\n",
    "\n",
    "### Load from memory and store to memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cutlass.cute.core._Tensor'>\n",
      "a_vec: tensor_value<vector<12xf32> o (3, 4)>\n",
      "b_vec: tensor_value<vector<12xf32> o (3, 4)>\n",
      "tensor(raw_ptr(0x0000000027138010: f32, generic, align<4>) o (3,4):(4,1), data=\n",
      "       [[ 2.000000,  2.000000,  2.000000,  2.000000, ],\n",
      "        [ 2.000000,  2.000000,  2.000000,  2.000000, ],\n",
      "        [ 2.000000,  2.000000,  2.000000,  2.000000, ]])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def load_and_store(res: cute.Tensor, a: cute.Tensor, b: cute.Tensor):\n",
    "    \"\"\"\n",
    "    Load data from memory and store the result to memory.\n",
    "\n",
    "    :param res: The destination tensor to store the result.\n",
    "    :param a: The source tensor to be loaded.\n",
    "    :param b: The source tensor to be loaded.\n",
    "    \"\"\"\n",
    "    print(type(a))\n",
    "    a_vec = a.load()\n",
    "    print(f\"a_vec: {a_vec}\")      # prints `a_vec: vector<12xf32> o (3, 4)`\n",
    "    b_vec = b.load()\n",
    "    print(f\"b_vec: {b_vec}\")      # prints `b_vec: vector<12xf32> o (3, 4)`\n",
    "    res.store(a_vec + b_vec)\n",
    "    cute.print_tensor(res)\n",
    "\n",
    "a = np.ones(12).reshape((3, 4)).astype(np.float32)\n",
    "b = np.ones(12).reshape((3, 4)).astype(np.float32)\n",
    "c = np.zeros(12).reshape((3, 4)).astype(np.float32)\n",
    "load_and_store(from_dlpack(c), from_dlpack(a), from_dlpack(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cutlass.cute.core._Tensor'>\n"
     ]
    },
    {
     "ename": "DSLRuntimeError",
     "evalue": "DSLRuntimeError: ðŸ’¥ðŸ’¥ðŸ’¥ Error during runtime code generation for function `load_and_store` ðŸ’¥ðŸ’¥ðŸ’¥",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:1055\u001b[39m, in \u001b[36mBaseDSL.generate_original_ir.<locals>.build_ir_module\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     result = \u001b[43mfuncBody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mir_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mir_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m     func.ReturnOp([])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_and_store\u001b[39m\u001b[34m(res, a, b)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(a))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m a_vec = \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma_vec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma_vec\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)      \u001b[38;5;66;03m# prints `a_vec: vector<12xf32> o (3, 4)`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/_mlir_helpers/op.py:31\u001b[39m, in \u001b[36mdsl_user_op.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     30\u001b[39m     loc = ir.Location.name(frame.f_code.co_name, childLoc=file_loc)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m res_or_list = \u001b[43mopFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_or_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/cute/core.py:1439\u001b[39m, in \u001b[36m_Tensor.load\u001b[39m\u001b[34m(self, loc, ip)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_static(\u001b[38;5;28mself\u001b[39m.shape):\n\u001b[32m-> \u001b[39m\u001b[32m1439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdynamic layout doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt support load\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1441\u001b[39m \u001b[38;5;28mself\u001b[39m._check_can_load_store()\n",
      "\u001b[31mValueError\u001b[39m: dynamic layout doesn't support load",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDSLRuntimeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m     res.store(a_vec + b_vec)\n\u001b[32m     16\u001b[39m     cute.print_tensor(res)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mload_and_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:429\u001b[39m, in \u001b[36mBaseDSL.jit_runner.<locals>.jit_runner_decorator.<locals>.jit_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjit_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    428\u001b[39m     func_ptr = BaseDSL._preprocess_and_execute(func)\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:1331\u001b[39m, in \u001b[36mBaseDSL._func\u001b[39m\u001b[34m(self, funcBody, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m \u001b[38;5;66;03m# Generate MLIR Context and start generating IR\u001b[39;00m\n\u001b[32m   1330\u001b[39m log().debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating MLIR for function \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_mlir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuncBody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcanonicalized_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_module_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcanonicalized_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mno_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompile_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:1160\u001b[39m, in \u001b[36mBaseDSL.generate_mlir\u001b[39m\u001b[34m(self, funcBody, kwargs, function_name, gpu_module_attrs, args, args_spec, pipeline, no_cache, compile_only, loc)\u001b[39m\n\u001b[32m   1155\u001b[39m exe_args, func_types = \u001b[38;5;28mself\u001b[39m.generate_mlir_function_types(\n\u001b[32m   1156\u001b[39m     funcBody, function_name, args, kwargs, args_spec\n\u001b[32m   1157\u001b[39m )\n\u001b[32m   1159\u001b[39m \u001b[38;5;66;03m# Generate original ir module and its hash value.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m module, module_hash, result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_original_ir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuncBody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_module_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;66;03m# dryrun is used to only generate IR\u001b[39;00m\n\u001b[32m   1173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.envar.dryrun:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:1077\u001b[39m, in \u001b[36mBaseDSL.generate_original_ir\u001b[39m\u001b[34m(self, ir, func, funcBody, kwargs, function_name, func_types, gpu_module_attrs, args, args_spec)\u001b[39m\n\u001b[32m   1075\u001b[39m \u001b[38;5;66;03m# Build IR module\u001b[39;00m\n\u001b[32m   1076\u001b[39m profiler = timer(enable=\u001b[38;5;28mself\u001b[39m.envar.jitTimeProfiling)\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m module, result = \u001b[43mprofiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_ir_module\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1078\u001b[39m module_hash = \u001b[38;5;28mself\u001b[39m.get_module_hash(module, function_name)\n\u001b[32m   1080\u001b[39m module = \u001b[38;5;28mself\u001b[39m.build_module(module, function_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/utils/timer.py:28\u001b[39m, in \u001b[36mtimer.<locals>.decorator.<locals>.func_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m enable:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[32m     31\u001b[39m     start = time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:1070\u001b[39m, in \u001b[36mBaseDSL.generate_original_ir.<locals>.build_ir_module\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1067\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m dsl_error\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m general_e:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;66;03m# Transform internal error to a DSL error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m DSLRuntimeError(\n\u001b[32m   1071\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ’¥ðŸ’¥ðŸ’¥ Error during runtime code generation for function `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncBody.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` ðŸ’¥ðŸ’¥ðŸ’¥\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1072\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeneral_e\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module, result\n",
      "\u001b[31mDSLRuntimeError\u001b[39m: DSLRuntimeError: ðŸ’¥ðŸ’¥ðŸ’¥ Error during runtime code generation for function `load_and_store` ðŸ’¥ðŸ’¥ðŸ’¥"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def load_and_store(res, a, b):\n",
    "    \"\"\"\n",
    "    Load data from memory and store the result to memory.\n",
    "\n",
    "    :param res: The destination tensor to store the result.\n",
    "    :param a: The source tensor to be loaded.\n",
    "    :param b: The source tensor to be loaded.\n",
    "    \"\"\"\n",
    "    print(type(a))\n",
    "    a_vec = a.load()\n",
    "    print(f\"a_vec: {a_vec}\")      # prints `a_vec: vector<12xf32> o (3, 4)`\n",
    "    b_vec = b.load()\n",
    "    print(f\"b_vec: {b_vec}\")      # prints `b_vec: vector<12xf32> o (3, 4)`\n",
    "    res.store(a_vec + b_vec)\n",
    "    cute.print_tensor(res)\n",
    "\n",
    "cutlass.mark\n",
    "load_and_store(*(torch.from_numpy(t) for t in (c, a, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register-Level Tensor Operations\n",
    "\n",
    "When writing kernel logic, various computations, transformations, slicing, etc., are performed on data loaded into registers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_value<vector<24xf32> o (4, 2, 3)> -> tensor_value<vector<12xf32> o (4, 3)>\n",
      "Is TensorSSA\n",
      "tensor(raw_ptr(0x0000000027b5d110: f32, generic, align<4>) o (4,3):(3,1), data=\n",
      "       [[ 3.000000,  4.000000,  5.000000, ],\n",
      "        [ 9.000000,  10.000000,  11.000000, ],\n",
      "        [ 15.000000,  16.000000,  17.000000, ],\n",
      "        [ 21.000000,  22.000000,  23.000000, ]])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def apply_slice(src: cute.Tensor, dst: cute.Tensor, indices: cutlass.Constexpr):\n",
    "    \"\"\"\n",
    "    Apply slice operation on the src tensor and store the result to the dst tensor.\n",
    "\n",
    "    :param src: The source tensor to be sliced.\n",
    "    :param dst: The destination tensor to store the result.\n",
    "    :param indices: The indices to slice the source tensor.\n",
    "    \"\"\"\n",
    "    src_vec = src.load()\n",
    "    dst_vec = src_vec[indices]\n",
    "    print(f\"{src_vec} -> {dst_vec}\")\n",
    "    if isinstance(dst_vec, cute.TensorSSA):\n",
    "        cute.printf(\"Is TensorSSA\")\n",
    "        dst.store(dst_vec)\n",
    "        cute.print_tensor(dst)\n",
    "\n",
    "    else:\n",
    "        dst[0] = dst_vec\n",
    "        cute.print_tensor(dst)\n",
    "\n",
    "def slice_1():\n",
    "    src_shape = (4, 2, 3)\n",
    "    dst_shape = (4, 3)\n",
    "    indices = (None, 1, None)\n",
    "\n",
    "    \"\"\"\n",
    "    a:\n",
    "    [[[ 0.  1.  2.]\n",
    "      [ 3.  4.  5.]]\n",
    "\n",
    "     [[ 6.  7.  8.]\n",
    "      [ 9. 10. 11.]]\n",
    "\n",
    "     [[12. 13. 14.]\n",
    "      [15. 16. 17.]]\n",
    "\n",
    "     [[18. 19. 20.]\n",
    "      [21. 22. 23.]]]\n",
    "    \"\"\"\n",
    "    a = np.arange(np.prod(src_shape)).reshape(*src_shape).astype(np.float32)\n",
    "    dst = np.random.randn(*dst_shape).astype(np.float32)\n",
    "    apply_slice(from_dlpack(a), from_dlpack(dst), indices)\n",
    "\n",
    "slice_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_value<vector<24xf32> o (4, 2, 3)> -> ?\n",
      "tensor(raw_ptr(0x000000001ff476b0: f32, generic, align<4>) o (1):(1), data=\n",
      "       [ 10.000000, ])\n"
     ]
    }
   ],
   "source": [
    "def slice_2():\n",
    "    src_shape = (4, 2, 3)\n",
    "    dst_shape = (1,)\n",
    "    indices = 10\n",
    "    a = np.arange(np.prod(src_shape)).reshape(*src_shape).astype(np.float32)\n",
    "    dst = np.random.randn(*dst_shape).astype(np.float32)\n",
    "    apply_slice(from_dlpack(a), from_dlpack(dst), indices)\n",
    "\n",
    "slice_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic Operations\n",
    "\n",
    "As we mentioned earlier, there're many tensor operations whose operands are `TensorSSA`. And they are all element-wise operations. We give some examples below.\n",
    "\n",
    "### Binary Operations\n",
    "\n",
    "For binary operations, the LHS operand is `TensorSSA` and the RHS operand can be either `TensorSSA` or `Numeric`. When the RHS is `Numeric`, it will be broadcast to a `TensorSSA`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(raw_ptr(0x0000000027eeaf30: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 3.000000, ],\n",
      "       [ 3.000000, ],\n",
      "       [ 3.000000, ])\n",
      "tensor(raw_ptr(0x0000000027eeaf30: f32, generic, align<4>) o (3):(1), data=\n",
      "       [-1.000000, ],\n",
      "       [-1.000000, ],\n",
      "       [-1.000000, ])\n",
      "tensor(raw_ptr(0x0000000027eeaf30: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 2.000000, ],\n",
      "       [ 2.000000, ],\n",
      "       [ 2.000000, ])\n",
      "tensor(raw_ptr(0x0000000027eeaf30: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 0.500000, ],\n",
      "       [ 0.500000, ],\n",
      "       [ 0.500000, ])\n",
      "tensor(raw_ptr(0x0000000027eeaf30: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 0.000000, ],\n",
      "       [ 0.000000, ],\n",
      "       [ 0.000000, ])\n",
      "tensor(raw_ptr(0x0000000027eeaf30: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 1.000000, ],\n",
      "       [ 1.000000, ],\n",
      "       [ 1.000000, ])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def binary_op_1(res: cute.Tensor, a: cute.Tensor, b: cute.Tensor):\n",
    "    a_vec = a.load()\n",
    "    b_vec = b.load()\n",
    "\n",
    "    add_res = a_vec + b_vec\n",
    "    res.store(add_res)\n",
    "    cute.print_tensor(res)        # prints [3.000000, 3.000000, 3.000000]\n",
    "\n",
    "    sub_res = a_vec - b_vec\n",
    "    res.store(sub_res)\n",
    "    cute.print_tensor(res)        # prints [-1.000000, -1.000000, -1.000000]\n",
    "\n",
    "    mul_res = a_vec * b_vec\n",
    "    res.store(mul_res)\n",
    "    cute.print_tensor(res)        # prints [2.000000, 2.000000, 2.000000]\n",
    "\n",
    "    div_res = a_vec / b_vec\n",
    "    res.store(div_res)\n",
    "    cute.print_tensor(res)        # prints [0.500000, 0.500000, 0.500000]\n",
    "\n",
    "    floor_div_res = a_vec // b_vec\n",
    "    res.store(floor_div_res)\n",
    "    cute.print_tensor(res)        # prints [0.000000, 0.000000, 0.000000]\n",
    "\n",
    "    mod_res = a_vec % b_vec\n",
    "    res.store(mod_res)\n",
    "    cute.print_tensor(res)        # prints [1.000000, 1.000000, 1.000000]\n",
    "\n",
    "\n",
    "a = np.empty((3,), dtype=np.float32)\n",
    "a.fill(1.0)\n",
    "b = np.empty((3,), dtype=np.float32)\n",
    "b.fill(2.0)\n",
    "res = np.empty((3,), dtype=np.float32)\n",
    "binary_op_1(from_dlpack(res), from_dlpack(a), from_dlpack(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 3.000000, ],\n",
      "       [ 3.000000, ],\n",
      "       [ 3.000000, ])\n",
      "tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=\n",
      "       [-1.000000, ],\n",
      "       [-1.000000, ],\n",
      "       [-1.000000, ])\n",
      "tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 2.000000, ],\n",
      "       [ 2.000000, ],\n",
      "       [ 2.000000, ])\n",
      "tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 0.500000, ],\n",
      "       [ 0.500000, ],\n",
      "       [ 0.500000, ])\n",
      "tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 0.000000, ],\n",
      "       [ 0.000000, ],\n",
      "       [ 0.000000, ])\n",
      "tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 1.000000, ],\n",
      "       [ 1.000000, ],\n",
      "       [ 1.000000, ])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def binary_op_2(res: cute.Tensor, a: cute.Tensor, c: cutlass.Constexpr):\n",
    "    a_vec = a.load()\n",
    "\n",
    "    add_res = a_vec + c\n",
    "    res.store(add_res)\n",
    "    cute.print_tensor(res)        # prints [3.000000, 3.000000, 3.000000]\n",
    "\n",
    "    sub_res = a_vec - c\n",
    "    res.store(sub_res)\n",
    "    cute.print_tensor(res)        # prints [-1.000000, -1.000000, -1.000000]\n",
    "\n",
    "    mul_res = a_vec * c\n",
    "    res.store(mul_res)\n",
    "    cute.print_tensor(res)        # prints [2.000000, 2.000000, 2.000000]\n",
    "\n",
    "    div_res = a_vec / c\n",
    "    res.store(div_res)\n",
    "    cute.print_tensor(res)        # prints [0.500000, 0.500000, 0.500000]\n",
    "\n",
    "    floor_div_res = a_vec // c\n",
    "    res.store(floor_div_res)\n",
    "    cute.print_tensor(res)        # prints [0.000000, 0.000000, 0.000000]\n",
    "\n",
    "    mod_res = a_vec % c\n",
    "    res.store(mod_res)\n",
    "    cute.print_tensor(res)        # prints [1.000000, 1.000000, 1.000000]\n",
    "\n",
    "a = np.empty((3,), dtype=np.float32)\n",
    "a.fill(1.0)\n",
    "c = 2.0\n",
    "res = np.empty((3,), dtype=np.float32)\n",
    "binary_op_2(from_dlpack(res), from_dlpack(a), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False]\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def binary_op_3(res: cute.Tensor, a: cute.Tensor, b: cute.Tensor):\n",
    "    a_vec = a.load()\n",
    "    b_vec = b.load()\n",
    "\n",
    "    gt_res = a_vec > b_vec\n",
    "    res.store(gt_res)\n",
    "\n",
    "    \"\"\"\n",
    "    ge_res = a_ >= b_   # [False, True, False]\n",
    "    lt_res = a_ < b_    # [True, False, True]\n",
    "    le_res = a_ <= b_   # [True, False, True]\n",
    "    eq_res = a_ == b_   # [False, False, False]\n",
    "    \"\"\"\n",
    "\n",
    "a = np.array([1, 2, 3], dtype=np.float32)\n",
    "b = np.array([2, 1, 4], dtype=np.float32)\n",
    "res = np.empty((3,), dtype=np.bool_)\n",
    "binary_op_3(from_dlpack(res), from_dlpack(a), from_dlpack(b))\n",
    "print(res)     # prints [False, True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "DSLRuntimeError",
     "evalue": "DSLRuntimeError: ðŸ’¥ðŸ’¥ðŸ’¥ Error during runtime code generation for function `binary_op_4` ðŸ’¥ðŸ’¥ðŸ’¥",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:1055\u001b[39m, in \u001b[36mBaseDSL.generate_original_ir.<locals>.build_ir_module\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     result = \u001b[43mfuncBody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mir_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mir_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m     func.ReturnOp([])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mbinary_op_4\u001b[39m\u001b[34m(res, a, b)\u001b[39m\n\u001b[32m      6\u001b[39m xor_res = a_vec ^ b_vec\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxor_res\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/_mlir_helpers/op.py:31\u001b[39m, in \u001b[36mdsl_user_op.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     30\u001b[39m     loc = ir.Location.name(frame.f_code.co_name, childLoc=file_loc)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m res_or_list = \u001b[43mopFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_or_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/cute/core.py:1475\u001b[39m, in \u001b[36m_Tensor.store\u001b[39m\u001b[34m(self, data, loc, ip)\u001b[39m\n\u001b[32m   1474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_elems != size(data.shape, loc=loc, ip=ip):\n\u001b[32m-> \u001b[39m\u001b[32m1475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1476\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlhs and rhs must have the same shape, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1477\u001b[39m     )\n\u001b[32m   1479\u001b[39m elem_mlir_type = cutlass_arith.element_type(data.dtype.mlir_type)\n",
      "\u001b[31mValueError\u001b[39m: lhs and rhs must have the same shape, but got (4,) and (3,)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDSLRuntimeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m b = np.array([\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m], dtype=np.int32)\n\u001b[32m     17\u001b[39m res = np.empty((\u001b[32m4\u001b[39m,), dtype=np.int32)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mbinary_op_4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_dlpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_dlpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_dlpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(res)     \u001b[38;5;66;03m# prints [3, 0, 7]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:429\u001b[39m, in \u001b[36mBaseDSL.jit_runner.<locals>.jit_runner_decorator.<locals>.jit_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjit_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    428\u001b[39m     func_ptr = BaseDSL._preprocess_and_execute(func)\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:1331\u001b[39m, in \u001b[36mBaseDSL._func\u001b[39m\u001b[34m(self, funcBody, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m \u001b[38;5;66;03m# Generate MLIR Context and start generating IR\u001b[39;00m\n\u001b[32m   1330\u001b[39m log().debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating MLIR for function \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_mlir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuncBody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcanonicalized_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_module_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcanonicalized_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mno_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompile_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:1160\u001b[39m, in \u001b[36mBaseDSL.generate_mlir\u001b[39m\u001b[34m(self, funcBody, kwargs, function_name, gpu_module_attrs, args, args_spec, pipeline, no_cache, compile_only, loc)\u001b[39m\n\u001b[32m   1155\u001b[39m exe_args, func_types = \u001b[38;5;28mself\u001b[39m.generate_mlir_function_types(\n\u001b[32m   1156\u001b[39m     funcBody, function_name, args, kwargs, args_spec\n\u001b[32m   1157\u001b[39m )\n\u001b[32m   1159\u001b[39m \u001b[38;5;66;03m# Generate original ir module and its hash value.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m module, module_hash, result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_original_ir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuncBody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_module_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;66;03m# dryrun is used to only generate IR\u001b[39;00m\n\u001b[32m   1173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.envar.dryrun:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:1077\u001b[39m, in \u001b[36mBaseDSL.generate_original_ir\u001b[39m\u001b[34m(self, ir, func, funcBody, kwargs, function_name, func_types, gpu_module_attrs, args, args_spec)\u001b[39m\n\u001b[32m   1075\u001b[39m \u001b[38;5;66;03m# Build IR module\u001b[39;00m\n\u001b[32m   1076\u001b[39m profiler = timer(enable=\u001b[38;5;28mself\u001b[39m.envar.jitTimeProfiling)\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m module, result = \u001b[43mprofiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_ir_module\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1078\u001b[39m module_hash = \u001b[38;5;28mself\u001b[39m.get_module_hash(module, function_name)\n\u001b[32m   1080\u001b[39m module = \u001b[38;5;28mself\u001b[39m.build_module(module, function_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/utils/timer.py:28\u001b[39m, in \u001b[36mtimer.<locals>.decorator.<locals>.func_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m enable:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[32m     31\u001b[39m     start = time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cutlass/cute_dsl/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/dsl.py:1070\u001b[39m, in \u001b[36mBaseDSL.generate_original_ir.<locals>.build_ir_module\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1067\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m dsl_error\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m general_e:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;66;03m# Transform internal error to a DSL error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m DSLRuntimeError(\n\u001b[32m   1071\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ’¥ðŸ’¥ðŸ’¥ Error during runtime code generation for function `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncBody.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` ðŸ’¥ðŸ’¥ðŸ’¥\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1072\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeneral_e\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module, result\n",
      "\u001b[31mDSLRuntimeError\u001b[39m: DSLRuntimeError: ðŸ’¥ðŸ’¥ðŸ’¥ Error during runtime code generation for function `binary_op_4` ðŸ’¥ðŸ’¥ðŸ’¥"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def binary_op_4(res: cute.Tensor, a: cute.Tensor, b: cute.Tensor):\n",
    "    a_vec = a.load()\n",
    "    b_vec = b.load()\n",
    "\n",
    "    xor_res = a_vec ^ b_vec\n",
    "    res.store(xor_res)\n",
    "\n",
    "    # or_res = a_vec | b_vec\n",
    "    # res.store(or_res)     # prints [3, 2, 7]\n",
    "\n",
    "    # and_res = a_vec & b_vec\n",
    "    # res.store(and_res)      # prints [0, 2, 0]\n",
    "\n",
    "a = np.array([1, 2, 3], dtype=np.int32)\n",
    "b = np.array([2, 2, 4], dtype=np.int32)\n",
    "res = np.empty((4,), dtype=np.int32)\n",
    "binary_op_4(from_dlpack(res), from_dlpack(a), from_dlpack(b))\n",
    "print(res)     # prints [3, 0, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unary Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(raw_ptr(0x0000000007fbd180: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 2.000000, ],\n",
      "       [ 2.000000, ],\n",
      "       [ 2.000000, ])\n",
      "tensor(raw_ptr(0x0000000007fbd180: f32, generic, align<4>) o (3):(1), data=\n",
      "       [-0.756802, ],\n",
      "       [-0.756802, ],\n",
      "       [-0.756802, ])\n",
      "tensor(raw_ptr(0x0000000007fbd180: f32, generic, align<4>) o (3):(1), data=\n",
      "       [ 16.000000, ],\n",
      "       [ 16.000000, ],\n",
      "       [ 16.000000, ])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def unary_op_1(res: cute.Tensor, a: cute.Tensor):\n",
    "    a_vec = a.load()\n",
    "\n",
    "    sqrt_res = cute.math.sqrt(a_vec)\n",
    "    res.store(sqrt_res)\n",
    "    cute.print_tensor(res)        # prints [2.000000, 2.000000, 2.000000]\n",
    "\n",
    "    sin_res = cute.math.sin(a_vec)\n",
    "    res.store(sin_res)\n",
    "    cute.print_tensor(res)        # prints [-0.756802, -0.756802, -0.756802]\n",
    "\n",
    "    exp2_res = cute.math.exp2(a_vec)\n",
    "    res.store(exp2_res)\n",
    "    cute.print_tensor(res)        # prints [16.000000, 16.000000, 16.000000]\n",
    "\n",
    "a = np.array([4.0, 4.0, 4.0], dtype=np.float32)\n",
    "res = np.empty((3,), dtype=np.float32)\n",
    "unary_op_1(from_dlpack(res), from_dlpack(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduction Operation\n",
    "\n",
    "The `TensorSSA`'s `reduce` method applies a specified reduction operation (`ReductionOp.ADD`, `ReductionOp.MUL`, `ReductionOp.MAX`, `ReductionOp.MIN`) starting with an initial value, and performs this reduction along the dimensions specified by the `reduction_profile.`. The result is typically a new `TensorSSA` with reduced dimensions or a scalar value if reduces across all axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is SSA tensor_value<vector<6xf32> o (2, 3)>: True\n",
      "Is SSA ArithValue(%18 = \"vector.reduction\"(%9, %17) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<add>}> : (vector<6xf32>, f32) -> f32): False\n",
      "21.000000\n",
      "Is SSA tensor_value<vector<2xf32> o (2,)>: True\n",
      "Is SSA tensor<ptr<f32, rmem, align<32>> o (2):(1)>: False\n",
      "tensor(raw_ptr(0x00007ffc57929740: f32, rmem, align<32>) o (2):(1), data=\n",
      "       [ 6.000000, ],\n",
      "       [ 15.000000, ])\n",
      "Is SSA tensor_value<vector<3xf32> o (3,)>: True\n",
      "tensor(raw_ptr(0x00007ffc57929760: f32, rmem, align<32>) o (3):(1), data=\n",
      "       [ 6.000000, ],\n",
      "       [ 8.000000, ],\n",
      "       [ 10.000000, ])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def checkSSA(t):\n",
    "    cute.printf(f\"Is SSA {t}: {isinstance(t, cute.TensorSSA)}\")\n",
    "\n",
    "@cute.jit\n",
    "def reduction_op(a: cute.Tensor):\n",
    "    \"\"\"\n",
    "    Apply reduction operation on the src tensor.\n",
    "\n",
    "    :param src: The source tensor to be reduced.\n",
    "    \"\"\"\n",
    "    a_vec = a.load()\n",
    "    checkSSA(a_vec)\n",
    "    red_res = a_vec.reduce(\n",
    "        cute.ReductionOp.ADD,\n",
    "        0.0,\n",
    "        reduction_profile=0\n",
    "    )\n",
    "    checkSSA(red_res)\n",
    "    cute.printf(red_res)        # prints 21.000000\n",
    "\n",
    "    red_res = a_vec.reduce(\n",
    "        cute.ReductionOp.ADD,\n",
    "       0.0,\n",
    "        reduction_profile=(None, 1)\n",
    "    )\n",
    "    checkSSA(red_res)\n",
    "    # We can't print the TensorSSA directly at this point, so we store it to a new Tensor and print it.\n",
    "    res = cute.make_fragment(red_res.shape, cutlass.Float32)\n",
    "    checkSSA(res)\n",
    "\n",
    "    res.store(red_res)\n",
    "    cute.print_tensor(res)        # prints [6.000000, 15.000000]\n",
    "\n",
    "    red_res = a_vec.reduce(\n",
    "        cute.ReductionOp.ADD,\n",
    "        1.0,\n",
    "        reduction_profile=(1, None)\n",
    "    )\n",
    "    checkSSA(red_res)\n",
    "    res = cute.make_fragment(red_res.shape, cutlass.Float32)\n",
    "    res.store(red_res)\n",
    "    cute.print_tensor(res)        # prints [6.000000, 8.000000, 10.000000]\n",
    "\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n",
    "reduction_op(from_dlpack(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cute-dsl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
